# Benchmark

This repositories focuses on our tentatives to build a Benchmark for the 3D assets evaluation. It was inspired from [T3Bench](https://github.com/THU-LYJ-Lab/T3Bench). The logic remains the same, we consider views of an icosahedron, but this time we're not using Chat-GPT 4.0 to evaluate the views but a BLIP model that is supposed to be specialised in image recognition.

⚠⚠⚠ Most script require a screen device and won't work on a screenless server.

## The python scripts

### 3DBench.py
This script was the first one coded to do an automated evaluation on 3D assets. It uses Blip models and different views given the chosen method. 
### 2e-3DBench.py

### 3DBench_cosine.py
Uses Blip model too but with a cosine dstance between two prompts
### Analyse_resultat_clip_score.ipynb  
This markdown was made to evaluate the performances of our 6 pipelines on the 100 3D assets generated. This is to compare with the human evaluation. 
### automatic_clip_bench.py & Evaluation_clip.csv
This script is the script that computed the performances of the 6 pipelines and save them in the Evaluation_clip.csv

## geometryCheck.py
Checks if a 3D asset is composed by only one component. 

## Images_and_prompt_similarity.py

## text_to_2D_or_3D_alignement.py

### Visualisation3D.py
This script was a first draft on a human evaluation to compare 2 assets generated by image-to-3D models such as CRM or Triposr. It creates a tkinter interface that opens the asset in a window and the user can spin it as he wants. 
Keep in mind that it was really a first draft, and after that we decided to use the .glb format for the images generated by the CRM..


## Other Files

### prompts.txt 
The prompt.txt file is a text file extracted from the single part of the dataset of prompts of [T3Bench](https://github.com/THU-LYJ-Lab/T3Bench). All our pipelines were executed over these 100 prompts.

### testPrompt.txt